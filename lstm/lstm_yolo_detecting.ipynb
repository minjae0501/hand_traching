{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.227 ğŸš€ Python-3.8.17 torch-1.12.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "Setup complete âœ… (8 CPUs, 7.9 GB RAM, 199.5/221.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hand_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(hand_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size = 65, hidden_size = 128, num_layers=1, batch_first=True) # xyz 63 + x_center, y_center = 65\n",
    "        self.lstm2 = nn.LSTM(input_size = 128, hidden_size = 256, num_layers=1, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size = 256, hidden_size = 512, num_layers=1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.lstm4 = nn.LSTM(input_size = 512, hidden_size = 256, num_layers=1, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size = 256, hidden_size = 128, num_layers=1, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size = 128, hidden_size = 64, num_layers=1, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.lstm7 = nn.LSTM(input_size = 64, hidden_size = 32, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:, -1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_path = './lstm_model.pth'\n",
    "\n",
    "lstm_model = hand_LSTM()\n",
    "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "lstm_model.eval()\n",
    "print(\"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO ê°ì²´ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# best_model = '/content/drive/MyDrive/Deep_learning_Team_PJ/hand_tracing/yolo_eraser/best.pt'\n",
    "best_model = 'best.pt'\n",
    "yolo_model = YOLO(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe ì† ê°ì§€ ëª¨ë“ˆ ì´ˆê¸°í™”\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(1) # for Mac\n",
    "# cap = cv2.VideoCapture(0) # for Windows\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.flip(frame, 1)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # YOLO ê°ì²´ ê°ì§€\n",
    "    box_results = yolo_model.predict(image)\n",
    "    boxes = box_results[0].boxes.xyxy.cpu()\n",
    "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
    "    confs = box_results[0].boxes.conf.float().cpu().tolist()\n",
    "\n",
    "\n",
    "    # ê°ì²´ ê°ì§€ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— í‘œì‹œ\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "              with torch.no_grad():\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "    cv2.imshow('Frame', image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
